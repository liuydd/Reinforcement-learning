# MP #1 - Bandit Algorithms

This assignment is designed for you to practice classical bandit algorithms with simulated environments.

- Part 1: Multi-armed Bandit Problem (42+10 points): get the basic idea of multi-armed bandit problem, implement classical algorithms like Upper Confidence Bound (UCB), Thompsom Sampling (TS) and Perturbed-history Exploration (PHE), and compare their performance in the simulated environment;
  
- Part 2: Contextual Linear Bandit Problem (58+10 points): get the basic idea of contextual linear bandit problem, implement the linear counterparts of the algorithms in Part 1, including LinUCB, LinTS and LinPHE, compare their performance in the simulated environments and summerize the influence that the shape of action set has on the performance.

# MP #2 - Markov Decision Process

This assignment is designed for you to practice classical solution methods to Markov Decision Processes (MDP).

- Part 1: Dynamic Programming (50 points): implement value iteration and policy iteration for MDP, and test your code using the provided grid world environment;

- Part 2: Model-free Control (50 points): implement off-policy Monte Carlo control and off-policy TD control (Q-learning) algorithms and test your code using the provided grid world environment.

# MP #3 - Temporal difference and policy gradient methods

This assignment is designed for you to practice the temporal difference and policy gradient method for reinforcement learning, one of the most popularly employed RL algorithms.

- Part 1: Implementating policy gradient algorithms (50 points): implement Reinforce and Actor-Critic algorithms and test your implementations using the provided grid world environment;

- Part 2: Fine-tuning and comparisons (50 points + 10 bonus points): fine-tune the hyperparameters in both algorithms to obtain the best performance, and compare the results with value-based algorithms that you have implemented in MP2.
